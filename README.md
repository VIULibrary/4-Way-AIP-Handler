


# 4-Way-AIP-Handler

Handles all your AIP needs: download, verify dates, filter, split, zip . . . the full treatment!

##     1: AIP DOWNLOADER

`python AIPDownloader.py` 

Download the Dspace SAF export that happens to be stored in an AWS S3 bucket, with an interruptable, resumable script. 
Progress is stored in `download_manifest.json`

Edit and rename `config.json.example` => `config.json` and add your connection details:

```python
{
"ACCESS_KEY" : "YOUR_KEY",
"SECRET_KEY" : "YOUR_SECRET_KEY",
"ENDPOINT_URL" : "YOUR_ENDPOINT",
"BUCKET_NAME" : "BUCKET_NAME",
"PREFIX" : "KEEP_EMPTY_FOR_ROOT",
"LOCAL_DIR" : "YOUR_LOCAL_OUTPUT_DIR"

}
``` 
If you downloaded a single 'zip of zips' rather than a large number of discreet zip files
unzip it for the AIP filter:

`ditto -x -k /Volumes/ExternalHDD/big_export.zip /Volumes/ExternalHDD/unzipped_AIPs`

Or if you prefer: 

`unzip your_export.zip -d /path/to/output_folder`


##    2: Date Checker 

Before running the AIP Filter, do a quick date analysis with:

`python dateChecker.py` 


```python
SOURCE_DIR = "YOUR_SOURCE_DIR"
CUTOFF_DATE = "2024-05-04"   #YYYY-MM-DD
```

This scans all AIPs and displays:

- Total AIPs found
- Extracted dateAvailable values
- Count of items newer than cutoff



##    3: AIP FILTER

`python AIPfilter.py`


Add your required details:
```python
# ======================
# CONFIG
# ======================
SOURCE_DIR = "/YOUR_SOURCE_DIR"
DEST_DIR = "/YOUR_DESTINATION_DIR/filtered_AIPs"
MANIFEST = "/YOUR_DESTINATION_DIR/manifest.csv"
CUTOFF_DATE = datetime(2024, 5, 4)  # copy and move AIPs > #YYYY-MM-DD
COPY_FILES = True                   # False for dry run
MAX_WORKERS = 2                     # HDD-friendly threading
```

This script: 

- Reads the `mets.xml` from each AIP ZIP  
- Extracts the **dateAvailable** and **handle**  
- Filters items based on a **cutoff date**  
- Copies passing AIPs to a **filtered folder**  
- Generates a **CSV manifest** with details for every AIP (clean it up for inclusion in the folders generated by the spitter - you likely just want to record data for the kept/copied files) 
- Includes a **progress bar + ETA** for monitoring 
- Shows a progress bar and ETA. 



Each AIP is logged in the CSV manifest with:

`| filename | handle | dateAvailable | size_bytes | status | reason |`


==========

- Copied and Moved files are copied to filtered_AIPs.
- SKIPPED files are listed with a reason (older than cutoff, missing METS, parse error, etc.).



##    4: Splitter 


Split your filtered results into multiple folders (zipped, if you like)

1. Edit the config:
   ```bash
   SOURCE_FOLDER="/path/to/filtered_AIPs"
   CSV_FILE="manifest.csv"
   BASE_NAME="2024-04-05to2025-10-31"
   NUM_SPLITS=2  # Number of archives to create
   ```

2. Run the splitter:
   ```bash
   ./splitter.sh
   ```

The script will:
- Analyze file sizes and distribute evenly across archives
- Create balanced splits (Archive A, B, C, etc.)
- Optionally create 7z compressed archives
- Include the CSV manifest in each archive





## Prerequisites

- Python 3.x  
- `tqdm` library for progress bar  

```pip3 install tqdm```


### Common Issues

**"No METS file" errors**
- Verify your AIP structure matches DSpace export format
- Check that ZIP files aren't corrupted

**"Date parse error" issues**  
- Review dateAvailable format in your METS files
- Modify the date parsing regex if using custom date formats

**Performance issues**
- Reduce `MAX_WORKERS` for HDDs
- Ensure adequate free disk space
- Close other disk-intensive applications

### Getting Help

1. Check the CSV manifest for detailed error messages
2. Run [dateChecker.py](dateChecker.py) to isolate date-related issues  
3. Test with a small subset of AIPs first

